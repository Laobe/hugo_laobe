{
  "title": "粗糙的分布式理解",
  "cells": [
    {
      "type": "text",
      "data": "<div><br></div><div>随着分布式系统的发展，已经面向大数据大计算时代的需求，各种分布式概念层出不穷，伴随的是出现了各种分布式系统(分布式系统，分布式存储，分布式锁，分布式事务，向量时钟)，各种分布式算法(RAFT,PAXOS,Consistent Hash, hash slot)，</div><div>各种分布式理论(CAP, 高可用高性能高并发)</div><div><br></div><div>于是就想粗糙的整理一下自己对这些分布式的认知和理解。</div><div><br></div><div>### 分布式是什么</div><div><br></div><div>不需多想，分布式就是系统分布在多台机器上，多台机器同时提供服务。 可以在性能，可用性，伸缩性，可维护等方面都提供了一定的优势</div><div><br></div><div>变来优势的同时，由于服务是部署在多台机器上的，在面对数据一致，分布平衡，容错性等方面也带来了一些挑战和问题，因此也引发出了一系列围绕分布式展开的系统实现，算法设计和理论定理等，下面就在简单谈谈我的一些粗糙的理解：</div><div><br></div><div>首先，围绕着分布式系统的特点，有一个定理成了分布式系统的一个理论基础，这就是CAP定理。在分布式系统中，一致性，可用性和分区容错性不能同时满足。</div><div><br></div><div>分布式系统可以为我们带来多台机器提供的资源，为我们面对大规模的计算和存储提供了条件。MapReduce就是比较早起设计实现的一种分布式计算，通过map将一个大规模的计算拆分成多个小规模的计算，分配到不同的机器上进行计算，然后用reduce将不同机器的计算聚合起来的一种分布式系统，后期出现的hadoop，spark也都是分布式计算的实现。</div><div><br></div><div>在MapReduce的同时期，谷歌还发布了GFS，一种分布式存储的实现，该系统是个master-chunkserver架构，将文件拆分为每个chunk，将每个节点定义为chunkserver，用来存储这些chunk，并且还有一个master用来存储所有chunk的基本信息，并且提供一个类似查询的接口供客户端调用以获取自己需要的文件在哪个chunk上，然后从该chunk上获取文件本身。这是一种有中心的架构，虽然这个中心可能会成为整个系统的瓶颈，抑或是导致整个系统不可用，但是有了这个中心后可以给整个系统的实现带来很多便利，从全局唯一出发，可以利用这个唯一中心来保证数据的一致性，在动态增减chunkserver或者平衡所有chunkserver的负载时候也可以通过这个中心来控制。并且由于metadata信息内容很少，在设计时将全部是存储metadata存储在master的内存中，响应速度会远远快于每一个chunk服务返回文件本身，在性能这一块在chunk一定量下的时候还不大可能会成为瓶颈。</div><div><br></div><div>在存储方面，除了文件存储之外，我们有时还可能需要结构化的存储供应用使用，谷歌在GFS之后又提出了BigTable，一个结构化数据的分布式存储，这个存储是架设在GFS之上，又因为是结构化的数据涉及到数据的读写修改，bigtable提供了单行的事务，在分布式事务中，该事务也就是分布式事务，我们都知道在传统的单机数据库中，事务的实现都离不开锁，锁行，锁表等等，在分布式事务的实现中也不例外，它需要的是<b>分布式锁</b>，面向所有节点提供全局的锁。bigtable依赖的是一个高可用持久化的<b>分布式锁</b>服务<b>Chubby，</b>当然分布式系统下的分布式锁自身实现肯定也是要分布式（有点绕），所以chubby使用了一个叫做<b>paxos</b>的一致性的算法，（在很长一段时间之后出来了另一个相对好理解的一致性算法，这个算法就是<b>raft)。</b>其实一致性算法本质的目地将数据同时存在多个节点中，已备在任何一个节点不可用的时候，整个服务都是可用的，因此会设计到主要的两个问题：1. 接受数据后怎么保证所有节点的数据都是一致的；2. 这些节点提供服务的逻辑是什么，当提供服务的节点不可用了怎么还能保证毫无感知的正常提供服务。（ 插个题外话在bigtable中还有一个很重要的技术影响者后来的很多分布式数据库，虽然其本身是个数据库索引技术而不是什么分布式相关的技术，但之后的很多分布式数据库都采用了该索引，这个就是sstable，也就是lsm树的前生。包括cassandra直接就使用了sstable，而leveldb就是完全的lsm树实现的kv存储，rocksdb又在leveldb之上建立了多线程高性能版本，并且成为了很多新型数据库的存储引擎。）<br></div><div><br></div><div>Dynamo 是现在aws中很重要的一个系统，他跟BigTable功能上类似，但在具体实现中有很大的不同，也采用了很多不一致的方法。同样是一个去中心的分布式服务，dynamo是利用配p2p(去中心化网络，好像在哪些奇怪的地方听过）来传播数据，利用consistent hash算法对各个节点的负载做平衡，在节点故障或者节点发现方面使用了gossip protocol（大家都爱的八卦，八卦的力量是无穷的）。 因为dynamo设计初衷是要满足亚马逊的电商系统，因此高可用是最最最主要的，基于上面的CAP理论，dynamo设计上选址了AP，<b>弱化了一致性</b>。因为分布式系统的读写都可能来自不同机器，因此采用了向量时钟（vector clocks在分布式系统中全局一致的时间）的方式来做处理读写冲突（这有点像我们在读写数据库时用的一个版本号的概念），然后在分区容错方面对于临时的出错（比如网络抖动等）采用了sloppy quorum策略来保证，对于永久的出错也就是服务挂了后重新起来的情况使用了一种Merkel trees（哈希树）的技术来恢复。上面说到一致性的时候之所以说是弱化了一致性而不是去掉了一致性的原因是在针对一致性这个概念中又衍生出了三种一致性概念，<b>强一致性，弱一致性和最终一致性</b>。而dynamo实现了最终一致性，因此dynamo还是存在数据一致的特性</div><div><img src=\"quiver-image-url/2FA78C8528FDC025A12025A8D9DEDB64.png\" alt=\"dynamo.png\" width=\"568\" height=\"648\"><br></div><div><br></div><div>对比上面这些历史的话，redis可谓诞生的就有点迟了(2009年发布），而且redis的本意是为了提供一个高效的内存服务，看上去似乎和分布式系统并没有什么关系。然后在redis 3.0.0版本开始，redis在社区的呼声下发布了redis cluster，定义了一堆slot，通过将这些slot平衡的分配到各个节点上，然后通过hash方式计算存储的key对应的slot，映射到相应的key上。在下节点时，将该节点的slot重新分配到所有别的节点上，或者在上节点时，将别的所有节点都取出一部分来slot分配给这个新的节点以做掉负载的平衡（<b>hash slot</b>）。这种架构是个<b>无中心的架构</b>，每个节点都对外提供服务，因为每个节点都知道所有节点拥有的slot，因此在需要提供的服务不在自己节点上时，会重定向到对应的节点上去。这就是一种很好的在内存上提供的分布式系统实现。</div><div><br></div><div>现在时间线推移到了2010年之后了，在分布式系统各个技术发展的推送下，比我们还老的关系型数据库也开始走向了分布式化，在分布式关系型数据库之前，随着存储的数据越来越大，我们常采用的方案是分库分表，看上去也像一个分布式模型，但这分库分表的模型更多的基于业务逻辑来的，而不是底层的存储，更像是一个人为维护着的系统。这些系统大都是基于谷歌的spanner和f1理论设计的。比较熟悉的分布式数据库如tidb等。</div><div><br></div><div>另外我们平时自己的业务系统中其实就有很多分布式系统，很多一些是将整个系统的处理拆分成多个单一的功能，通过消息同步等方式来串联整个系统。在将一个系统拆分成多个单一的功能通过消息传递的时候，就涉及到了每个单一功能执行的成功与否，在某些功能执行失败的情况下，别的功能是否要回退等问题。也就是如何保证分布式事务，这里就又提出了2PC，3PC等概念了。</div><div><br></div><div>### 总结</div><div>为了让所有机器尽可能正确高效稳定的提供服务，分布式系统从宏观架构上看，区分有无中心的架构模型。并且解决的问题主要包括各个节点的数据是否一致，或者是各个节点提供的服务是否平衡。还有在某些节点出现问题的情况下，整个系统如果自恢复。并且多个节点是如何都能提供正确的服务。</div><div><br></div><div>分布式系统是建立在单机系统之上的，因此在面对分布式问题的同时，也会包含很多单机系统的问题，或者是对这个单机系统解决方案改善为适合分布式系统的（比如事务等等）。因此分布式系统是一个很庞大的研究对象，会涉及到方方面面，也会一直变化着有新的技术引入，或者旧的理论不成立等等。是一个涉及面很广很动态的的课题。</div><div><br></div>"
    }
  ]
}